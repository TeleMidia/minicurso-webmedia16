\documentclass{SBCbookchapter}
\input{chapter-header.tex}
\title{Programando aplicações multimídia no GStreamer}
\author{%
  Guilherme F\null.~Lima,
  Rodrigo C\null.\,M.~Santos,
  Roberto G\null.\,~de~A.~Azevedo
}
\begin{document}
\maketitle
\vskip-1.525\baselineskip\strut
\begin{abstract}
  \input{chapter-abstract-en}
\end{abstract}
\begin{resumo}
  Este minicurso é uma introdução ao GStreamer, um dos principais
  \en{frameworks} de código livre/aberto para processamento de dados
  multimídia.  Começamos apresentando o GStreamer, sua arquitetura e modelo
  de programação baseado em \en{dataflow}, e em seguida, adotamos uma
  abordagem prática.  Partindo de um exemplo inicial, um \en{player} de
  vídeo, introduzimos cada conceito da API~C básica do GStreamer e o
  implementamos sobre o exemplo, incrementando-o, de forma que ao final do
  minicurso obtemos um \en{player} de vídeo interativo completo, com suporte
  às operações usuais de reprodução de vídeo (\en{start}, \en{stop},
  \en{seek}, \en{fast-forward} e~\en{rewind}).  Discutimos também filtros de
  amostras---elementos que manipulam as amostras de áudio e vídeo.
  Apresentamos os diversos filtros disponíveis nativamente no GStreamer e
  mostramos como estender o \en{framework} criando um \en{plugin} com um
  filtro simples que manipula amostras de vídeo.  O único pré-requisito para
  o minicurso é um conhecimento básico da linguagem de programação~C\null.
  Ao final do minicurso, esperamos que os participantes tenham uma visão
  geral do GStreamer, e estejam aptos a criar aplicações simples e explorar
  os recursos mais avançados do \en{framework}.
\end{resumo}


\section{Introdução}
\label{sec:intro}

O GStreamer~\cite{gstreamer} é um dos principais \en{frameworks} de código
livre/aberto para processamento de dados multimídia.  Além de robusto e
flexível, ele suporta diversos formatos de áudio e vídeo, e é amplamente
utilizado na indústria e na academia~\cite{gstreamer-apps}.
O \en{framework} em si consiste de um conjunto de bibliotecas~C e
ferramentas relacionadas.  Neste minicurso, apresentamos tanto a parte
conceitual quanto a prática do \en{framework}.

Na parte conceitual, discutimos o modelo de computação \en{dataflow} no qual
o GStreamer se baseia, e que também é adotado por outros \en{frameworks} e
linguagens multimídia, por exemplo, DirectShow~\cite{Chatterjee-A-1997},
Pure Data~\cite{Puckette-M-S-2007}, CLAM~\cite{Amatriain-X-2008},
ChucK~\cite{Wang-G-2003}, Faust~\cite{Orlarey-Y-2009}, etc.  Nesse modelo,
uma aplicação multimídia estrutura-se como um grafo em que os nós são
elementos processadores e as arestas representam conexões entre elementos
por onde fluem as amostras de áudio e vídeo e dados de controle.  O modelo
de \en{dataflow} é particularmente interessante para multimídia porque
possibilita implementações naturalmente paralelas, modulares e escaláveis.

Na parte prática, apresentamos os principais conceitos da
API~C~\cite{Kernighan-B-W-1988} básica do GStreamer~1.10, sua versão estável
mais atual, e ilustramos o uso dessa API a partir da construção de um
reprodutor (\en{player}) de vídeo.  Apesar de aqui estarmos interessados
apenas na reprodução (decodificação e apresentação) de fluxos de mídia, essa
mesma API pode ser utilizada para capturar fluxos de áudio e vídeo,
codificá-los e transmiti-los na rede.  O GStreamer possui uma grande
variedade de componentes para tratar cada uma dessas fases de processamento
e, portanto, pode ser usado para construir diversos tipos de aplicações
multimídia tais como editores de vídeo, transcodificadores, transmissores de
fluxos de mídia, \en{players} de mídia e motores de renderização de
linguagens multimídia.

O restante do capítulo está organizado da seguinte forma.  Na
Seção~\ref{sec:ola} apresentamos uma versão preliminar do exemplo base do
minicurso: um \en{player} de vídeo simples porém funcional.  Na
Seção~\ref{sec:dissec} discutimos o que está por trás do código
aparentemente simples do exemplo anterior e reconstruímos o mesmo exemplo
usando a API básica (baixo nível) do GStreamer; essa versão reconstruída é o
ponto de partida dos incrementos posteriores.  Na Seção~\ref{sec:filtros}
apresentamos os principais filtros de áudio e vídeo disponíveis no GStreamer
e discutimos como integrá-los ao exemplo.  Na Seção~\ref{sec:e/s}
adicionamos suporte a eventos de teclado e mouse ao \en{player} de vídeo, e
mostramos como injetar ou extrair amostras de uma aplicação GStreamer.  Na
Seção~\ref{sec:ops} discutimos a teoria por trás das operações usuais de
controle de reprodução (\en{pause}, \en{seek}, \en{fast-foward}
e~\en{rewind}) e implementamos o suporte a cada uma delas no exemplo.  Na
Seção~\ref{sec:plugins} apresentamos a arquitetura de \en{plugins} do
GStreamer e mostramos como implementar e integrar ao exemplo um
\emph{plugin} simples contendo um elemento que manipula amostras de vídeo.
Finalmente, na Seção~\ref{sec:conclusao} discutimos funcionalidades
avançadas do \en{framework} e listamos algumas referências para estudos
posteriores.

Antes de programar nossa primeira aplicação GStreamer porém, é preciso
entender o modelo de computação no qual o \en{framework} se baseia.  No
restante dessa seção apresentamos esse modelo, \emph{dataflow}, e discutimos
a forma como ele é instanciado no GStreamer.


\subsection*{O modelo \en{dataflow} e sua instanciação no GStreamer}

No modelo de computação \en{dataflow} os dados são processados enquanto
``fluem'' através de uma rede.  Essa rede estrutura-se como um grafo
dirigido em que os nós representam elementos de processamento, ou atores, e
as arestas representam conexões unidirecionais por onde fluem os dados.  Os
atores recebem dados através de suas portas de entrada e emitem dados
através de suas porta de saída.  Além disso, conceitualmente, atores são
ativados por entrada, isto é, executam sempre que os dados necessários estão
disponíveis em suas portas de entrada.  Um \en{pipeline} é um \en{dataflow}
em que os dados fluem através das arestas na mesma ordem em que foram
produzidos~\cite{Kahn-G-1977,Lee-E-A-1995}.  O modelo de computação
\en{dataflow}, e em especial, o modelo de \en{pipeline} é interessante para
multimídia porque aproxima a estrutura real do sistema da sua descrição
abstrata, idealizada na forma de diagrama de blocos~\cite{Yviquel-H-2014}.
Além disso, o modelo de \en{dataflow} induz implementações flexíveis e
eficientes já que ele é naturalmente paralelo, modular e escalável:
(i)~paralelo porque conceitualmente os atores executam independentemente uns
dos outros; (ii)~modular porque a lógica de processamento está encapsulada
nos atores e pode ser reusada em diversos pontos do \en{dataflow};
e~(iii)~escalável porque a estrutura é naturalmente composicional (um
\en{dataflow} pode ser visto como um ator e vice versa).

A Figura~\ref{fig:pipe-tipico} apresenta o leiaute de um \en{pipeline}
típico para processamento multimídia.  O leiaute nesse caso é uma
simplificação do \en{pipeline} de uma aplicação GStreamer que reproduz um
vídeo.  Os nós da figura representam atores e as arestas representam as
conexões por onde fluem as amostras de áudio e vídeo e dados de controle.
Na terminologia do GStreamer os atores do são chamados de ``elementos'' e as
portas de~``\en{pads}''.  Há dois tipos de \en{pads}, \en{sink pads} e
\en{source pads}.  As \en{sink pads} são as portas de entrada através das
quais o elemento consome dados, e as \en{source pads} são as portas de saída
através das quais o elemento produz dados.  Os elementos são classificados
de acordo com o tipo de suas \en{pads}.  Elementos produtores (\en{sources})
possuem apenas \en{source pads}, elementos processadores possuem ambos os
tipos, \en{source pads} e \en{sink pads}, e elementos consumidores
(\en{sinks}) possuem apenas \en{sink pads}.  Consequentemente, produtores
apenas produzem dados, processadores consomem e produzem dados, e
consumidores apenas consomem dados.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node (filesrc) [element] {filesrc};
    \coordinate [above=.5\hdim of filesrc] (A);
    \coordinate [below=.5\hdim of filesrc] (B);
    %%
    \node (oggdemux) [element, right of=filesrc] {oggdemux};
    \coordinate [right of=oggdemux] (C);
    %%
    \node (vorbisdec) [element] at (C|-A) {vorbisdec};    
    \node (alsasink) [element, right of=vorbisdec] {alsasink};
    \node (theoradec) [element] at (C|-B) {theoradec};
    \node (xvimagesink) [element, right of=theoradec] {xvimagesink};
    %%
    \draw [->, arrow] (filesrc) -- (oggdemux);
    \coordinate (X) at ($(oggdemux.east)+(0,.25\hdim)$);
    \coordinate (Y) at ($(oggdemux.east)+(0,-.25\hdim)$);
    \draw [->, arrow] (X) -- node [arrowlabel, above] {A} ++(\odim/3,0)
                          -- ($(vorbisdec.west)-(\odim/3,0)$)
                          -- (vorbisdec.west);
    \draw [->, arrow] (Y) -- node [arrowlabel, below] {V} ++(\odim/3,0)
                          -- ($(theoradec.west)-(\odim/3,0)$)
                          -- (theoradec.west);
    \draw [->, arrow] (vorbisdec) -- (alsasink);
    \draw [->, arrow] (theoradec) -- (xvimagesink);
  \end{tikzpicture}
  \caption{Um \en{pipeline} GStreamer que reproduz um arquivo de vídeo Ogg.}
  \label{fig:pipe-tipico}
\end{figure}
\vskip-\baselineskip

Na figura acima há um elemento produtor (``filesrc''), três processadores
(``oggdemux'', ``vorbisdec'' e ``theoradec'') e dois consumidores
(``alsasink'' e ``xvimagesink'').  O elemento ``filesrc'' possui uma única
\en{source pad} que está conectada à \en{sink pad} do elemento subsequente,
``oggdemux''.  Ou seja, os dados produzidos pelo elemento ``filesrc'' fluem
através da sua \en{source pad} para a \en{sink pad} do elemento
``oggdemux''.  No diagrama essa conexão entre as \en{pads} é representada
pela aresta entre os elementos ``filesrc'' e ``oggdemux''.  Similarmente, as
demais arestas denotam conexões entre \en{source pads} e \en{sink pads}.

O processo de desenvolvimento de uma aplicação GStreamer com \en{pipeline}
estático (cuja topologia não muda em tempo de execução) é relativamente
simples.  Basta instanciar os elementos necessários, interconectar suas
\en{pads} e iniciar \en{pipeline} resultante.  O \en{pipeline} da
Figura~\ref{fig:pipe-tipico}, por exemplo, após iniciado opera da seguinte
forma.
\begin{enumerate}
\item O elemento ``filesrc'' lê um arquivo Ogg armazenado no sistema de
  arquivos e escreve o fluxo de bytes resultantes na sua \en{source pad}.
  Ogg~\cite{ogg-rfc-3533} é um formato para multiplexação de fluxos de
  áudio, vídeo e texto.  Vamos assumir que arquivo Ogg nesse caso contém
  apenas dois fluxos multiplexados: um fluxo de áudio (sequência de amostras
  de áudio) codificado no formato Vorbis~\cite{vorbis}, e um fluxo de vídeo
  (sequência de quadros de imagem) codificado no formato
  Theora~\cite{theora}.
\item O elemento ``oggdemux'' lê da sua \en{sink pad} um fluxo de bytes
  codificado no formato Ogg, demultiplexa-o e escreve os fluxos Vorbis
  (áudio) e Theora (vídeo) resultantes nas \en{source pads} correspondentes.
  Na figura~\ref{fig:pipe-tipico}, a \en{source pad} que recebe o fluxo
  de áudio codificado possui o rótulo~``A'' e a \en{source pad} que recebe o
  fluxo de vídeo codificado possui o rótulo~``V''.
\item O elemento ``vorbisdec'' lê da sua \en{sink pad} um fluxo de bytes
  codificado no formato Vorbis, decodifica-o e escreve o fluxo de áudio~PCM
  resultante na sua \en{source pad}.  Um fluxo de áudio PCM é o que chamamos
  de ``áudio descomprimido'' (\en{raw}), ou seja, é uma sequência de
  amostras obtidas via \en{pulse-code modulation} que representa o sinal
  analógico original.
\item O elemento ``theoradec'' opera de maneira análoga.  Ele lê da sua
  \en{sink pad} um fluxo de bytes codificado no formato Theora, decodifica-o
  e escreve o fluxo de vídeo descomprimido (\en{raw}) resultante na sua
  \en{source pad}.  Um fluxo de vídeo descomprimido é uma sequência de
  amostras (quadros) de vídeo em cada quadro é uma matriz de \en{pixels}
  codificados em algum modelo de cor.  No modelo de cor RGB
  (\en{red-green-blue}), por exemplo, cada \en{pixel} é codificado como uma
  sequência de três inteiros que representam tonalidades de vermelho, verde
  e azul.
\item O elemento ``alsasink'' lê um fluxo de áudio descomprimido da sua
  \en{sink pad} e utiliza a biblioteca ALSA~\cite{alsa} para reproduzir as
  amostras do fluxo nos alto-falantes.
\item O elemento ``xvimagesink'' lê um fluxo de vídeo descomprimido da sua
  \en{sink pad} e utiliza a biblioteca X11~\cite{x11} para reproduzir os
  quadros do fluxo na tela.
\end{enumerate}

A função de um \en{pipeline}, isto é, o que ele faz ou computa, é o
resultado da combinação da função dos seus elementos que, conceitualmente,
operam em paralelo.  O \en{pipeline} anterior, portanto, (1)~lê um arquivo
Ogg, (2)~demultiplexa-o, (3--4)~decodifica os fluxos de áudio e vídeo
resultantes e~(5--6) reproduz os fluxos decodificados nos dispositivos de
saída correspondentes (alto-falantes e tela).  E tudo isso acontece
conceitualmente em paralelo.  Ou seja, podemos assumir que enquanto os
\en{sinks} ``alsasink'' e ``xvimagesink'' estão exibindo amostras, o
\en{source} ``filesrc'' está lendo bytes do disco, o demultiplexador
``oggdemux'' está demultiplexando dados Ogg e os decodificadores
``vorbisdec'' e ``theoradec'' estão decodificando dados Vorbis e Theora.

A descrição anterior seria suficiente se estivéssemos interessados apenas em
exibir as amostras decodificadas o mais rápido possível.  Mas esse não é
caso.  Queremos ``tocar'' o vídeo original em tempo real, ou seja,
reproduzi-lo nas mesmas condições em que ele foi gravado (amostrado).  Nesse
caso, para que o vídeo seja reproduzido corretamente, suas amostras de áudio
e vídeo devem ser exibidas na taxa correta.  Valores típicos para essas
taxas são~44100Hz para amostras de áudio e~30Hz para amostras de vídeo.  Ou
seja, a cada~22{,}67\textmu{s} uma nova amostra de áudio deve ser enviada ao
dispositivo de saída de áudio, e a cada~33{,}33ms uma nova amostra de vídeo
deve ser enviada ao dispositivo de saída de~vídeo.

No GStreamer, em geral, são os elementos \en{sink} os responsáveis por
controlar as taxas de exibição de amostras.  Por exemplo, no \en{pipeline}
da Figura~\ref{fig:pipe-tipico}, para manter a taxa de reprodução
necessária, os \en{sinks} ``alsasink'' e ``xvimagesink'' armazenam as
amostras recebidas numa fila interna, e exibem-nas (consomem-nas) apenas no
momento adequado.  Já os outros elementos operam livres, isto é, consomem e
produzem dados em taxas arbitrárias.  Se durante a execução os elementos que
precedem os \en{sinks} (fluxo acima) operarem rápido o bastante, as filas
dos \en{sinks} nunca ficarão vazias e todas as amostras serão exibidas no
momento correto, mas esse nem sempre é o caso.

Se os elementos fluxo acima operarem abaixo da taxa de consumo dos
\en{sinks}, pode ser que a fila interna de um dos \en{sinks} fique vazia e,
caso chegue o momento de exibir uma amostra, o \en{sink} simplesmente não
tenha o que exibir, causando interrupções ou saltos na reprodução (amostras
``atrasadas'' quando chegarem serão descartadas).  Há ainda o problema
inverso, se os elementos fluxo acima operarem muito acima da taxa de consumo
dos \en{sinks}, pode ser que a capacidade da fila interna seja excedida e
amostras sejam~perdidas.  Para evitar ambos os problemas, esvaziamento ou
estouro das filas, ou mesmo para limitar o uso de CPU, os \en{sinks}
normalmente enviam fluxo acima eventos de QoS (\en{quality of service}) que
indicam o quão atrasada ou adiantada está cada amostra que chega.  Dessa
forma, os elementos fluxo acima (produtores e processadores) podem capturar
os eventos e usar a informação de retardo para ajustar a sua taxa de
operação.

Dois tipos de dados trafegam através das conexões de um \en{pipeline}
GStreamer: segmentos de dados (\en{buffers}) e eventos (\en{events}).  Os
\en{buffers} carregam segmentos do conteúdo processado entre \en{pads} (por
exemplo, amostras de áudio e vídeo codificadas ou \en{raw}) e fluem
exclusivamente na direção das conexões, ou seja, de \en{source pads} para
\en{sink pads}.  Já os eventos carregam informação de controle; eles também
fluem entre conexões, mas podem percorrê-las em ambos os sentidos: fluxo
abaixo (\en{downstream}), de \en{source pads} para \en{sink pads}, ou fluxo
acima \en{upstream}, de \en{sink pads} para \en{source pads}.  Além de
eventos de QoS, elementos podem emitir eventos de EOS (\en{end-of-stream})
que indicam o fim do fluxo, eventos de \en{seek} que indicam avanços ou
retrocessos no fluxo e eventos de \en{flush} que sinalizam que \en{caches}
internos devem ser descarregados.

\en{Buffers} e eventos percorrem as conexões em paralelo.  Ou seja,
conceitualmente cada conexão entre as \en{pads} dos elementos pode ser
entendida como consistindo de dois canais, um canal unidirecional para
\en{buffers} e outro canal bidirecional para eventos.
A Figura~\ref{fig:conexao} ilustra a estrutura interna, conceitual de uma
conexão.  Na figura,``B'' é o canal de \en{buffers} e ``E'' é o canal de
eventos.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node [cylinder, thick, draw=black!60, cylinder uses custom fill,
           cylinder end fill=black!30, cylinder body fill=black!20,
           minimum height=\wdim, minimum width=\hdim] (c) {};
    \coordinate (x) at ($(c.before top|-c.east)+(0,.15\hdim)$);
    \coordinate (y) at ($(c.before top|-c.east)-(0,.15\hdim)$);
    \coordinate (x0) at ($(c.west|-x)+(.25pt,0)$);
    \coordinate (y0) at ($(c.west|-y)+(.25pt,0)$);
    \draw [->,arrow] (x) -- node [arrowlabel, above, pos=.4] {B} ++(\odim,0);
    \draw [arrow] (x0) -- ++(-\odim,0);
    \draw [->,arrow] (y) -- node [arrowlabel, below, pos=.4] {E} ++(\odim,0);
    \draw [->,arrow] (y0) -- ++(-\odim,0);
  \end{tikzpicture}
  \caption{Estrutura conceitual de uma conexão entre \en{pads} no
    GStreamer.}
  \label{fig:conexao}
\end{figure}

A discussão do modelo de \en{dataflow} e da sua instanciação no GStreamer se
encerra aqui.  Na maior parte do tempo, programar no GStreamer consiste em
montar \en{pipelines} e controlar seu funcionamento.  Até agora discutimos
de maneira abstrata os conceitos envolvidos nessa montagem e controle.  Está
na hora de ver esses conceitos na~prática.


\section{Olá mundo: Tocando um vídeo}
\label{sec:ola}

Nosso objetivo nessa seção é usar o GStreamer para tocar um vídeo.  Para tal
poderíamos implementar o \en{pipeline} da Figura~\ref{fig:pipe-tipico}, mas
há uma maneira mais simples: basta usar o elemento ``playbin''.
A Listagem~\ref{lst:hello} apresenta um programa~C que faz exatamente isso.

\lstinputlisting[
style=display,
caption={Tocando um vídeo no GStreamer usando o elemento ``playbin''.},
label={lst:hello},
]{src/hello.c}

Vejamos o propósito de cada linha da Listagem~\ref{lst:hello}.  A linha~1
inclui as declarações da GLib~\cite{glib}, a biblioteca de portabilidade do
projeto GNOME~\cite{gnome}, e a linha~2 inclui as declarações do
GStreamer---o GStreamer depende do \en{framework} GObject~\cite{gobject} da
GLib para programação orientada a objetos em~C\null.  Na listagem, as
chamadas com prefixo ``\C{g_}'' ou ``\C{G_}'' e referem-se à funções e
macros da GLib, e as chamadas com prefixo ``\C{gst_}'' ou ``\C{GST_}'' e as
declarações com prefixo ``\C{Gst}'' referem-se à funções, macros e tipos do
GStreamer.

A chamada~\C{gst_init}, linha~12, inicializa o GStreamer e trata os
argumentos com prefixo ``-\,-gst-'' em \C{argv}.

A próxima chamada, linha~14, cria um elemento ``playbin'' que é também o
\en{pipeline} da aplicação.  No GStreamer, o \en{pipeline} é ele próprio um
elemento, de fato, um elemento que contém outros elementos mas não possui
\en{pads}.  Tais elementos contêiner são chamados de ``\en{bins}''.
A função \C{gst_element_factory_make} aloca e retora um novo elemento
(\C{GstElement}) do tipo especificado.  O primeiro parâmetro da função é o
nome da fábrica do tipo e o segundo parâmetro (opcional) é o nome a ser
atribuído ao elemento retornado.  Nesse caso, a chamada cria e retorna um
elemento do tipo ``playbin'' chamado ``hello''.  Um elemento ``playbin''
nada mais é do que um \en{pipeline} que se autoconfigura.  Dada uma URI,
assim que é inciado o ``playbin'' determina o tipo do conteúdo da URI e
constrói um \en{pipeline} apropriado para reproduzi-lo.

A chamada \C{g_assert_nonnull}, linha~15, é uma asserção que garante que a
chamada anterior funcionou corretamente, isto é, retornou um elemento válido
(não nulo).

A chamada \C{gst_filename_to_uri}, linha~17, constrói uma URI a partir do
caminho de um arquivo.  Nesse caso, ``bunny.ogg'' é caminho do arquivo de
vídeo Ogg que queremos tocar.  O vídeo em si é o curta de animação ``Big
Buck Bunny''~\cite{bunny} produzido pelo Blender Institute e licenciado via
Creative Commons.

A chamada \C{g_object_set}, linha~19, atribui a URI criada anteriormente à
propriedade ``uri'' do elemento ``playbin''.  Note que \C{g_object_set} é
uma função da GObject.  Todo elemento (\C{GstElement}) é também um objeto
(\C{GObject}), e todo objeto possui propriedades cujos valores podem ser
obtidos via \C{g_object_get} e alterados via \C{g_object_set}.
A propriedade ``uri'' do elemento ``playbin'' contém a URI do conteúdo de
mídia a ser reproduzido.

A chamada \C{g_free}, linha~20, libera a \en{string} alocada na linha~16.
Nesse ponto a \en{string} já não é mais necessária pois foi copiada pela
chamada \C{g_object_set} anterior.

A chamada \C{gst_element_set_state}, linha~22, inicia o \en{pipeline} da
aplicação, isto é, transiciona elemento ``playbin'' do seu estado inicial
\en{null} (\C{GST_STATE_NULL}) para o estado \en{playing}
(\C{GST_STATE_PLAYING}).  Na Seção~\ref{sec:dissec}, vamos discutir em
detalhes as consequências internas dessa transição.  Por enquanto, basta
dizer que nesse ponto o elemento ``playbin''~(1) inspeciona o conteúdo
apontado pela URI configurada, (2)~instancia e interconecta os elementos
necessários para reproduzir esse conteúdo, (3)~inicia a sua reprodução numa
\en{thread} separada e~(4) devolve o controle à \en{thread} principal da
aplicação.

A chamada seguinte, \C{g_assert} na linha~23, é uma asserção que garante que
a requisição de transição de estado anterior foi bem sucedida.  Após essa
linha, podemos assumir que o vídeo está tocando e que aplicação possui pelo
menos duas \en{threads}: a \en{thread} principal, que está prestes a
executar a linha~23, e uma ou mais \en{threads} secundárias, que operam o
\en{pipeline}.  A \en{thread} principal é chamada de ``\en{thread} da
aplicação'' e as \en{threads} secundárias são chamadas de ``\en{streaming
  threads}''.  As \en{streaming threads} se comunicam com a \en{thread} da
aplicação através de mensagens assíncronas postadas num barramento
(\en{bus}) associado ao \en{pipeline}.  Essas mensagens informam a
\en{thread} da aplicação sobre acontecimentos internos do \en{pipeline}, por
exemplo, mudança de estado de elementos, fim do fluxo, erros, \en{warnings},
etc., e permitem que aplicação reaja da maneira apropriada.

A chamada \C{gst_element_get_bus}, linha~25, obtém uma referência para o
barramento de mensagens (\en{bus}) do \en{pipeline} do ``playbin''.

A próxima chamada, linha~26, bloqueia a \en{thread} da aplicação até que uma
mensagem de erro ou de EOS (\en{end-of-stream}, ``fim do fluxo'') seja
postada no barramento.  Ou seja, a \en{thread} da aplicação aguarda nessa
chamada até um erro aconteça ou até que o vídeo seja reproduzido
completamente.  A função \C{gst_bus_timed_pop_filtered} recebe o \en{bus}, o
tempo máximo de espera e a máscara dos tipos das mensagens a serem
aguardadas, e bloqueia até que o tempo máximo seja atingido ou até que uma
mensagem de um dos tipos esperados seja postada no \en{bus}.  A função
retorna \C{NULL} caso o tempo máximo seja atingido ou retorna uma referência
para a mensagem recebida.  Na Listagem~\ref{lst:hello}, estamos aguardando
por um tempo ilimitado (\C{GST_TIME_CLOCK_NONE}) uma mensagem de erro
(\C{GST_MESSAGE_ERROR}) ou uma mensagem de EOS (\C{GST_MESSAGE_EOS}), que
após a chamada é armazenada na variável \C{msg}.

A chamada \C{gst_message_unref},


\section{Dissecando o exemplo anterior}
\label{sec:dissec}

No Gstreamer, todo elemento incluindo o
\en{pipeline} possui um estado, que pode ser nulo (\en{null}), pronto
(\en{ready}), pausado (\en{paused}) ou tocando (\en{playing}).  No estado
inicial, \en{null}, o elemento não possui recursos alocados.  No estado
\en{ready}, o elemento aloca recursos globais que não dependem do conteúdo a
ser processado.  No estado \en{paused}, o elemento aloca recursos que
dependem do conteúdo a ser processado e se prepara para processá-lo.
Finalmente, no estado \en{playing} o elemento inicia o processamento.

Para chegar do estado \en{null} (estado inicial) ao estado \en{playing}
(estado operante) o elemento tem que passar primeiro pelo estados \en{ready}
e \en{paused}, nessa ordem.  De forma análoga, para sair do estado
\en{playing} e voltar ao estado inicial \en{null}, o elemento tem que passar
pelos estados \en{paused} e \en{ready}.  A Figura~\ref{fig:estados-elt}
apresenta máquina de estados de um elemento GStreamer, ou seja, os estados e
transições possíveis.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[node distance=.65\wdim+\odim]
    \node [state] (NULL) {null};
    \node [state, right of=NULL] (READY) {ready};
    \node [state, right of=READY] (PAUSED) {paused};
    \node [state, right of=PAUSED] (PLAYING) {playing};
    \draw [->] (NULL) to [bend left=25] (READY);
    \draw [->] (READY) to [bend left=25] (PAUSED);
    \draw [->] (PAUSED) to [bend left=25] (PLAYING);
    \draw [->] (PLAYING) to [bend left=25] (PAUSED);
    \draw [->] (PAUSED) to [bend left=25] (READY);
    \draw [->] (READY) to [bend left=25] (NULL);
  \end{tikzpicture}
  \caption{Máquina de estados de um elemento GStreamer (\C{GstElement}).}
  \label{fig:estados-elt}
\end{figure}


\section{Filtros}
\label{sec:filtros}


\section{Entrada e saída}
\label{sec:e/s}


\section{\en{Pause}, \en{seek}, \en{fast-forward}
  e~\en{rewind}}
\label{sec:ops}


\section{Plugins}
\label{sec:plugins}


\section{Conclusão}
\label{sec:conclusao}


\bibliographystyle{plain}
\bibliography{bib}
\end{document}

%  LocalWords:  API start stop fast-forward plugin CLAM ChucK Faust ii iii
%  LocalWords:  bib GStreamer frameworks dataflow player seek rewind Pure
%  LocalWords:  framework DirectShow players fast-foward plugins pads sink
%  LocalWords:  source sources sinks Ogg filesrc oggdemux vorbisdec pad PCM
%  LocalWords:  theoradec alsasink xvimagesink bytes Vorbis Theora raw RGB
%  LocalWords:  demultiplexa-o pulse-code modulation pixels red-green-blue
%  LocalWords:  ALSA CPU QoS quality of service buffers events downstream
%  LocalWords:  upstream EOS end-of-stream flush LocalWords playbin GLib
%  LocalWords:  GNOME GObject gst GST init argv bins element factory make
%  LocalWords:  GstElement hello assert nonnull filename bunny ogg Big Buck
%  LocalWords:  Blender Institute Creative Commons object set get free null
%  LocalWords:  string state PLAYING Gstreamer ready paused playing thread
%  LocalWords:  threads streaming bus
