\documentclass{SBCbookchapter}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
\usepackage[english,brazilian]{babel}
\usepackage[protrusion=true,expansion]{microtype}
\usepackage{hyperref}
\hypersetup{colorlinks=true,allcolors=black}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{float}
\usepackage{natbib}
\setlength{\bibsep}{0.0pt}
%%
\usepackage{tikz}
\usetikzlibrary{arrows,calc,positioning}
\newdimen\hdim                  % element height
\newdimen\wdim                  % element width
\newdimen\odim                  % offset distance
\hdim=2.25em
\wdim=2.36\hdim
\odim=\hdim
\tikzstyle{element}=[
  draw=black!60,
  font={\scriptsize\sffamily},
  inner sep=0pt,
  minimum height=\hdim,
  minimum width=\wdim,
  outer sep=0pt,
  rectangle,
  rounded corners=\wdim/10,
  text centered,
  thick,
  top color=white,
  bottom color=black!20,
]
\tikzstyle{arrow}=[
  ->,
  >=stealth,
  color=black!60,
  draw=black!60,
  shorten >=.5pt,
  thick,
]
\tikzstyle{arrowlabel}=[
  color=black!60,
  font={\tiny\sffamily\bfseries},
]
\tikzset{node distance=\wdim+\odim}
%%
\def\en#1{\foreignlanguage{english}{\emph{#1}}}
%%
\title{Programando aplicações multimídia no GStreamer}
\author{%
  Guilherme F\null.~Lima,
  Rodrigo C\null.\,M.~Santos,
  Roberto G\null.\,~de~A.~Azevedo
}
\begin{document}
\maketitle
\vskip-1.525\baselineskip\strut
\begin{abstract}
  \input{chapter-abstract-en}
\end{abstract}
\begin{resumo}
  Este minicurso é uma introdução ao GStreamer, um dos principais
  \en{frameworks} de código livre/aberto para processamento de dados
  multimídia.  Começamos apresentando o GStreamer, sua arquitetura e modelo
  de programação baseado em \en{dataflow}, e em seguida, adotamos uma
  abordagem prática.  Partindo de um exemplo inicial, um \en{player} de
  vídeo, introduzimos cada conceito da API~C básica do GStreamer e o
  implementamos sobre o exemplo, incrementando-o, de forma que ao final do
  minicurso obtemos um \en{player} de vídeo interativo completo, com suporte
  às operações usuais de reprodução de vídeo (\en{start}, \en{stop},
  \en{seek}, \en{fast-forward} e~\en{rewind}).  Discutimos também filtros de
  amostras---elementos que manipulam as amostras de áudio e vídeo.
  Apresentamos os diversos filtros disponíveis nativamente no GStreamer e
  mostramos como estender o \en{framework} criando um \en{plugin} com um
  filtro simples que manipula amostras de vídeo.  O único pré-requisito para
  o minicurso é um conhecimento básico da linguagem de programação~C\null.
  Ao final do minicurso, esperamos que os participantes tenham uma visão
  geral do GStreamer, e estejam aptos a criar aplicações simples e explorar
  os recursos mais avançados do \en{framework}.
\end{resumo}


\section{Introdução}
\label{sec:intro}

O GStreamer~\cite{gstreamer} é um dos principais \en{frameworks} de código
livre/aberto para processamento de dados multimídia.  Além de robusto e
flexível, ele suporta diversos formatos de áudio e vídeo, e é amplamente
utilizado na indústria e na academia~\cite{gstreamer-apps}.
O \en{framework} em si consiste de um conjunto de bibliotecas~C e
ferramentas relacionadas.  Neste minicurso, apresentamos tanto a parte
conceitual quanto a prática do \en{framework}.

Na parte conceitual, discutimos o modelo de computação \en{dataflow} no qual
o GStreamer se baseia, e que também é adotado por outros \en{frameworks} e
linguagens multimídia, por exemplo, DirectShow~\cite{Chatterjee-A-1997},
Pure Data~\cite{Puckette-M-S-2007}, CLAM~\cite{Amatriain-X-2008},
ChucK~\cite{Wang-G-2003}, Faust~\cite{Orlarey-Y-2009}, etc.  Nesse modelo,
uma aplicação multimídia estrutura-se como um grafo em que os nós são
elementos processadores e as arestas representam conexões entre elementos
por onde fluem as amostras de áudio e vídeo e dados de controle.  O modelo
de \en{dataflow} é particularmente interessante para multimídia porque
possibilita implementações naturalmente paralelas, modulares e escaláveis.

Na parte prática, apresentamos os principais conceitos da API~C básica do
GStreamer~1.10, sua versão estável mais atual, e ilustramos o uso dessa API
a partir da construção de um reprodutor (\en{player}) de vídeo.  Apesar de
aqui estarmos interessados apenas na reprodução (decodificação e
apresentação) de fluxos de mídia, essa mesma API pode ser utilizada para
capturar fluxos de áudio e vídeo, codificá-los e transmiti-los na rede.
O GStreamer possui uma grande variedade de componentes para tratar cada uma
dessas fases de processamento e, portanto, pode ser usado para construir
diversos tipos de aplicações multimídia tais como editores de vídeo,
transcodificadores, transmissores de fluxos de mídia, \en{players} de mídia
e motores de renderização de linguagens multimídia.

O restante do capítulo está organizado da seguinte forma.  Na
Seção~\ref{sec:ola} apresentamos uma versão preliminar do exemplo base do
minicurso: um \en{player} de vídeo simples porém funcional.  Na
Seção~\ref{sec:dissec} discutimos o que está por trás do código
aparentemente simples do exemplo anterior e reconstruímos o mesmo exemplo
usando a API básica (baixo nível) do GStreamer; essa versão reconstruída é o
ponto de partida dos incrementos posteriores.  Na Seção~\ref{sec:filtros}
apresentamos os principais filtros de áudio e vídeo disponíveis no GStreamer
e discutimos como integrá-los ao exemplo.  Na Seção~\ref{sec:e/s}
adicionamos suporte a eventos de teclado e mouse ao \en{player} de vídeo, e
mostramos como injetar ou extrair amostras de uma aplicação GStreamer.  Na
Seção~\ref{sec:ops} discutimos a teoria por trás das operações usuais de
controle de reprodução (\en{pause}, \en{seek}, \en{fast-foward}
e~\en{rewind}) e implementamos o suporte a cada uma delas no exemplo.  Na
Seção~\ref{sec:plugins} apresentamos a arquitetura de \en{plugins} do
GStreamer e mostramos como implementar e integrar ao exemplo um
\emph{plugin} simples contendo um elemento que manipula amostras de vídeo.
Finalmente, na Seção~\ref{sec:conclusao} discutimos funcionalidades
avançadas do \en{framework} e listamos algumas referências para estudos
posteriores.

Antes de programar nossa primeira aplicação GStreamer porém, é preciso
entender o modelo de computação no qual o \en{framework} se baseia.  No
restante dessa seção apresentamos esse modelo, \emph{dataflow}, e discutimos
a forma como ele é instanciado no GStreamer.


\subsection*{O modelo \en{dataflow} e sua instanciação no GStreamer}

No modelo de computação \en{dataflow} os dados são processados enquanto
``fluem'' através de uma rede.  Essa rede estrutura-se como um grafo
dirigido em que os nós representam elementos de processamento, ou atores, e
as arestas representam conexões unidirecionais por onde fluem os dados.  Os
atores recebem dados através de suas portas de entrada e emitem dados
através de suas porta de saída.  Além disso, conceitualmente, atores são
ativados por entrada, isto é, executam sempre que os dados necessários estão
disponíveis em suas portas de entrada.  Um \en{pipeline} é um \en{dataflow}
em que os dados fluem através das arestas na mesma ordem em que foram
produzidos~\cite{Kahn-G-1977,Lee-E-A-1995}.  O modelo de computação
\en{dataflow}, e em especial, o modelo de \en{pipeline} é interessante para
multimídia porque aproxima a estrutura real do sistema da sua descrição
abstrata, idealizada na forma de diagrama de blocos~\cite{Yviquel-H-2014}.
Além disso, o modelo de \en{dataflow} induz implementações flexíveis e
eficientes já que ele é naturalmente paralelo, modular e escalável:
(i)~paralelo porque conceitualmente os atores executam independentemente uns
dos outros; (ii)~modular porque a lógica de processamento está encapsulada
nos atores e pode ser reusada em diversos pontos do \en{dataflow};
e~(iii)~escalável porque a estrutura é naturalmente composicional (um
\en{dataflow} pode ser visto como um ator e vice versa).

A Figura~\ref{fig:pipe-tipico} apresenta o leiaute de um \en{pipeline}
típico para processamento multimídia.  O leiaute nesse caso é uma
simplificação do \en{pipeline} de uma aplicação GStreamer que reproduz um
vídeo.  Os nós da figura representam atores e as arestas representam as
conexões por onde fluem as amostras de áudio e vídeo e dados de controle.
Na terminologia do GStreamer os atores do são chamados de ``elementos'' e as
portas de~``\en{pads}''.  Há dois tipos de \en{pads}, \en{sink pads} e
\en{source pads}.  As \en{sink pads} são as portas de entrada através das
quais o elemento consome dados, e as \en{source pads} são as portas de saída
através das quais o elemento produz dados.  Os elementos são classificados
de acordo com o tipo de suas \en{pads}.  Elementos produtores (\en{sources})
possuem apenas \en{source pads}, elementos processadores possuem ambos os
tipos, \en{source pads} e \en{sink pads}, e elementos consumidores
(\en{sinks}) possuem apenas \en{sink pads}.  Consequentemente, produtores
apenas produzem dados, processadores consomem e produzem dados, e
consumidores apenas consomem dados.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node (filesrc) [element] {filesrc};
    \node [coordinate, above=.5\hdim of filesrc] (A) {};
    \node [coordinate, below=.5\hdim of filesrc] (B) {};
    %%
    \node (oggdemux) [element, right of=filesrc] {oggdemux};
    \node [coordinate, right of=oggdemux] (C) {};
    %%
    \node (vorbisdec) [element] at (C|-A) {vorbisdec};    
    \node (alsasink) [element, right of=vorbisdec] {alsasink};
    \node (theoradec) [element] at (C|-B) {theoradec};
    \node (xvimagesink) [element, right of=theoradec] {xvimagesink};
    %%
    \draw [arrow] (filesrc) -- (oggdemux);
    \node [coordinate] (X) at ($(oggdemux.east)+(0,.25\hdim)$) {};
    \node [coordinate] (Y) at ($(oggdemux.east)+(0,-.25\hdim)$) {};
    \draw [arrow] (X) -- node [arrowlabel, above] {A} ++(\odim/3,0)
                      -- ($(vorbisdec.west)-(\odim/3,0)$)
                      -- (vorbisdec.west);
    \draw [arrow] (Y) -- node [arrowlabel, below] {V} ++(\odim/3,0)
                      -- ($(theoradec.west)-(\odim/3,0)$)
                      -- (theoradec.west);
    \draw [arrow] (vorbisdec) -- (alsasink);
    \draw [arrow] (theoradec) -- (xvimagesink);
  \end{tikzpicture}
  \caption{Um \en{pipeline} GStreamer que reproduz um arquivo de vídeo Ogg.}
  \label{fig:pipe-tipico}
\end{figure}
\vskip-\baselineskip

Na figura acima há um elemento produtor (``filesrc''), três processadores
(``oggdemux'', ``vorbisdec'' e ``theoradec'') e dois consumidores
(``alsasink'' e ``xvimagesink'').  O elemento ``filesrc'' possui uma única
\en{source pad} que está conectada à \en{sink pad} do elemento subsequente,
``oggdemux''.  Ou seja, os dados produzidos pelo elemento ``filesrc'' fluem
através da sua \en{source pad} para a \en{sink pad} do elemento
``oggdemux''.  No diagrama essa conexão entre as \en{pads} é representada
pela aresta entre os elementos ``filesrc'' e ``oggdemux''.  Similarmente, as
demais arestas denotam conexões entre \en{source pads} e \en{sink pads}.

O processo de desenvolvimento de uma aplicação GStreamer com \en{pipeline}
estático (cuja topologia não muda em tempo de execução) é relativamente
simples.  Basta instanciar os elementos necessários, interconectar suas
\en{pads} e iniciar \en{pipeline} resultante.  O \en{pipeline} da
Figura~\ref{fig:pipe-tipico}, por exemplo, após iniciado opera da
seguinte forma.
\begin{enumerate}
\item O elemento ``filesrc'' lê um arquivo Ogg armazenado no sistema de
  arquivos e escreve o fluxo de bytes resultantes na sua \en{source pad}.
  Ogg~\cite{ogg-rfc-3533} é um formato para multiplexação de fluxos de
  áudio, vídeo e texto.  Vamos assumir que arquivo Ogg nesse caso contém
  apenas dois fluxos multiplexados: um fluxo de áudio (sequência de amostras
  de áudio) codificado no formato Vorbis~\cite{vorbis}, e um fluxo de vídeo
  (sequência de quadros de imagem) codificado no formato
  Theora~\cite{theora}.
\item O elemento ``oggdemux'' lê da sua \en{sink pad} um fluxo de bytes
  codificado no formato Ogg, demultiplexa-o e escreve os fluxos Vorbis
  (áudio) e Theora (vídeo) resultantes nas \en{source pads} correspondentes.
  Na figura~\ref{fig:pipe-tipico}, a \en{source pad} que recebe o fluxo
  de áudio codificado possui o rótulo~``A'' e a \en{source pad} que recebe o
  fluxo de vídeo codificado possui o rótulo~``V''.
\item O elemento ``vorbisdec'' lê da sua \en{sink pad} um fluxo de bytes
  codificado no formato Vorbis, decodifica-o e escreve o fluxo de áudio~PCM
  resultante na sua \en{source pad}.  Um fluxo de áudio PCM é o que chamamos
  de áudio descomprimido (\en{raw}), ou seja, é uma sequência de amostras
  obtidas via \en{pulse-code modulation} que representa o sinal analógico
  original.
\item O elemento ``theoradec'' opera de maneira análoga.  Ele lê da sua
  \en{sink pad} um fluxo de bytes codificado no formato Theora, decodifica-o
  e escreve o fluxo de vídeo descomprimido (\en{raw}) resultante na sua
  \en{source pad}.  Um fluxo de vídeo descomprimido é uma sequência de
  amostras (quadros) de vídeo em cada quadro é uma matriz de \en{pixels}
  codificados em algum modelo de cor.  No modelo de cor RGB
  (\en{red-green-blue}), por exemplo, cada \en{pixel} é codificado como uma
  sequência de três inteiros que representam tonalidades de vermelho, verde
  e azul.
\item O elemento ``alsasink'' lê um fluxo de áudio descomprimido da sua
  \en{sink pad} e utiliza a biblioteca ALSA~\cite{alsa} para reproduzir as
  amostras do fluxo nos alto-falantes.
\item O elemento ``xvimagesink'' lê um fluxo de vídeo descomprimido da sua
  \en{sink pad} e utiliza a biblioteca X11~\cite{x11} para reproduzir os
  quadros do fluxo na tela.
\end{enumerate}

A função de um \en{pipeline}, isto é, o que ele faz ou computa, é o
resultado da combinação da função dos seus elementos que, conceitualmente,
operam em paralelo.  O \en{pipeline} anterior, portanto, (1)~lê um arquivo
Ogg, (2)~demultiplexa-o, (3--4)~decodifica os fluxos de áudio e vídeo
resultantes e~(5--6) reproduz os fluxos decodificados nos dispositivos de
saída correspondentes (alto-falantes e tela).  E tudo isso acontece
conceitualmente em paralelo.  Ou seja, podemos assumir que enquanto os
\en{sinks} ``alsasink'' e ``xvimagesink'' estão exibindo amostras, o
\en{source} ``filesrc'' está lendo bytes do disco, o demultiplexador
``oggdemux'' está demultiplexando dados Ogg e os decodificadores
``vorbisdec'' e ``theoradec'' estão decodificando dados Vorbis e Theora.

A descrição anterior seria suficiente se estivéssemos interessados apenas em
exibir as amostras decodificadas o mais rápido possível.  Mas esse não é
caso.  Queremos ``tocar'' o vídeo original em tempo real, ou seja,
reproduzi-lo nas mesmas condições em que ele foi gravado (amostrado).  Nesse
caso, para que o vídeo seja reproduzido corretamente, suas amostras de áudio
e vídeo devem ser exibidas na taxa correta.  Valores típicos para essas
taxas são~44100Hz para amostras de áudio e~30Hz para amostras de vídeo.  Ou
seja, a cada~22{,}67\textmu{s} uma nova amostra de áudio deve ser enviada ao
dispositivo de saída de áudio, e a cada~33{,}33ms uma nova amostra de vídeo
deve ser enviada ao dispositivo de saída de~vídeo.

No GStreamer, em geral, são os elementos \en{sink} os responsáveis por
controlar as taxas de exibição de amostras.  Por exemplo, no \en{pipeline}
da Figura~\ref{fig:pipe-tipico}, para manter a taxa de reprodução
necessária, os \en{sinks} ``alsasink'' e ``xvimagesink'' armazenam as
amostras recebidas numa fila interna, e exibem-nas (consomem-nas) apenas no
momento adequado.  Já os outros elementos operam livres, isto é, consomem e
produzem dados em taxas arbitrárias.  Se durante a execução os elementos que
precedem os \en{sinks} operarem rápido o bastante, as filas dos \en{sinks}
nunca ficarão vazias e todas as amostras serão exibidas no momento correto.
Mas esse nem sempre é o caso.

Se os elementos fluxo acima operarem abaixo da taxa de consumo dos
\en{sinks}, pode ser que a fila interna de um dos \en{sinks} fique vazia e,
caso chegue o momento de exibir uma amostra, o \en{sink} simplesmente não
tenha o que exibir, causando interrupções ou saltos na reprodução (amostras
``atrasadas'' quando chegarem serão descartadas).  Há ainda o problema
inverso, se os elementos fluxo acima operarem muito acima da taxa de consumo
dos \en{sinks}, pode ser que a capacidade da fila interna seja excedida e
amostras sejam~perdidas.  Para evitar ambos os problemas, esvaziamento ou
estouro das filas, ou mesmo para limitar o uso de CPU, os \en{sinks}
normalmente enviam fluxo acima eventos de QoS (\en{quality of service}) que
indicam o quão atrasada ou adiantada está cada amostra que chega.  Os
elementos fluxo acima (produtores e processadores) podem capturar esses
eventos e usar a informação de retardo para ajustar a sua taxa de operação.

De fato, dois tipos de dados trafegam através das conexões de um
\en{pipeline} GStreamer: segmentos de dados (\en{buffers}) e eventos
(\en{events}).  Os \en{buffers} carregam segmentos do conteúdo processado
entre \en{pads} (por exemplo, amostras de áudio e vídeo codificadas ou
\en{raw}) e fluem exclusivamente na direção das conexões, ou seja, de
\en{source pads} para \en{sink pads}.  Já os eventos carregam informação de
controle; eles também fluem entre conexões, mas podem percorrê-las em ambos
os sentidos: fluxo abaixo (\en{downstream}), de \en{source pads} para
\en{sink pads}, ou fluxo acima \en{upstream}, de \en{sink pads} para
\en{source pads}.  Além de eventos de QoS, elementos podem emitir eventos de
EOS (\en{end-of-stream}) que indicam o fim do fluxo, eventos de \en{seek},
que indicam avanços ou retrocessos no fluxo, e eventos de \en{flush} que
sinalizam que os elementos fluxo abaixo devem descarregar \en{caches}
internos.



\section{Olá mundo: Tocando um vídeo}
\label{sec:ola}

% Nesta seção...  Poderíamos usar implementar pipeline da figura para fazer
% nosso "olá mundo"---e é exatamente isso que vamos fazer na seção~3---mas
% há um jeito mais fácil.  Elementos encapsulam
% funcionalidade... gst-inspect


\section{Dissecando o exemplo anterior}
\label{sec:dissec}


\section{Filtros}
\label{sec:filtros}


\section{Entrada e saída}
\label{sec:e/s}


\section{\en{Pause}, \en{seek}, \en{fast-forward}
  e~\en{rewind}}
\label{sec:ops}


\section{Plugins}
\label{sec:plugins}


\section{Conclusão}
\label{sec:conclusao}


\bibliographystyle{plain}
\bibliography{bib}
\end{document}

%  LocalWords:  API start stop fast-forward plugin CLAM ChucK Faust ii iii
%  LocalWords:  bib
